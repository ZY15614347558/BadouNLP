# -*- coding: utf-8 -*-
import torch
import re
import numpy as np
from collections import defaultdict
from transformers import BertModel, BertConfig, BertTokenizer
from torch.utils.data import Dataset, DataLoader

"""
基于BERT的命名实体识别模型及评估器
"""

class NERDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, max_length):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # 标签映射
        self.label2id = {
            "B-LOCATION": 0,
            "B-ORGANIZATION": 1,
            "B-PERSON": 2,
            "B-TIME": 3,
            "I-LOCATION": 4,
            "I-ORGANIZATION": 5,
            "I-PERSON": 6,
            "I-TIME": 7,
            "O": 8
        }
        self.id2label = {v: k for k, v in self.label2id.items()}
        
    def __len__(self):
        return len(self.sentences)
    
    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        labels = self.labels[idx]
        
        # 分词并转换为id
        inputs = self.tokenizer.encode_plus(
            sentence,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # 处理标签，考虑特殊token
        input_ids = inputs['input_ids'][0].tolist()
        encoded_labels = []
        for i, token_id in enumerate(input_ids):
            if i == 0 or i == len(input_ids) - 1:  # [CLS]和[SEP]
                encoded_labels.append(self.label2id["O"])
            else:
                encoded_labels.append(labels[i-1] if i-1 < len(labels) else self.label2id["O"])
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': inputs['attention_mask'][0],
            'labels': torch.tensor(encoded_labels, dtype=torch.long)
        }

class BertNERModel(torch.nn.Module):
    def __init__(self, num_labels, model_name='bert-base-chinese'):
        super(BertNERModel, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.dropout = torch.nn.Dropout(0.1)
        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, num_labels)
        
    def forward(self, input_ids, attention_mask=None, labels=None):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        sequence_output = outputs[0]
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)
        
        outputs = (logits,) + outputs[2:]  # 添加hidden states和attention if they are here
        
        if labels is not None:
            loss_fct = torch.nn.CrossEntropyLoss()
            # 只计算非特殊token的损失
            active_loss = attention_mask.view(-1) == 1
            active_logits = logits.view(-1, logits.shape[-1])[active_loss]
            active_labels = labels.view(-1)[active_loss]
            loss = loss_fct(active_logits, active_labels)
            outputs = (loss,) + outputs
        
        return outputs  # (loss), scores, (hidden_states), (attentions)

class Evaluator:
    def __init__(self, config, logger):
        self.config = config
        self.logger = logger
        
        # 初始化tokenizer和模型
        self.tokenizer = BertTokenizer.from_pretrained(config["model_name"])
        self.model = BertNERModel(num_labels=9, model_name=config["model_name"])
        
        # 加载验证数据
        self.valid_data = self.load_data(config["valid_data_path"], config)
        
        # 加载模型权重
        if config["model_path"]:
            self.model.load_state_dict(torch.load(config["model_path"]))
            self.logger.info(f"已加载模型权重: {config['model_path']}")
        
        if torch.cuda.is_available():
            self.model = self.model.cuda()
    
    def load_data(self, data_path, config):
        # 假设data_path指向一个文件，每行格式为: 句子\t标签序列
        sentences = []
        labels = []
        
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if not line.strip():
                    continue
                parts = line.strip().split('\t')
                if len(parts) != 2:
                    continue
                
                sentence = parts[0]
                label_str = parts[1]
                
                # 转换标签为id
                label2id = {
                    "B-LOCATION": 0,
                    "B-ORGANIZATION": 1,
                    "B-PERSON": 2,
                    "B-TIME": 3,
                    "I-LOCATION": 4,
                    "I-ORGANIZATION": 5,
                    "I-PERSON": 6,
                    "I-TIME": 7,
                    "O": 8
                }
                
                label_ids = [label2id.get(label, 8) for label in label_str.split()]
                sentences.append(sentence)
                labels.append(label_ids)
        
        # 创建数据集和数据加载器
        dataset = NERDataset(sentences, labels, self.tokenizer, config["max_length"])
        dataloader = DataLoader(dataset, batch_size=config["batch_size"], shuffle=False)
        
        return dataloader
    
    def eval(self, epoch=None):
        if epoch is not None:
            self.logger.info(f"开始测试第{epoch}轮模型效果：")
        else:
            self.logger.info("开始测试模型效果：")
            
        self.stats_dict = {"LOCATION": defaultdict(int),
                           "TIME": defaultdict(int),
                           "PERSON": defaultdict(int),
                           "ORGANIZATION": defaultdict(int)}
        
        self.model.eval()
        
        for batch_data in self.valid_data:
            if torch.cuda.is_available():
                batch_data = {k: v.cuda() for k, v in batch_data.items()}
            
            input_ids = batch_data['input_ids']
            attention_mask = batch_data['attention_mask']
            labels = batch_data['labels']
            
            with torch.no_grad():
                outputs = self.model(input_ids, attention_mask=attention_mask)
                pred_logits = outputs[0]
                pred_labels = torch.argmax(pred_logits, dim=-1)
            
            self.write_stats(labels, pred_labels, input_ids, attention_mask)
        
        self.show_stats()
        return
    
    def write_stats(self, true_labels, pred_labels, input_ids, attention_mask):
        batch_size = true_labels.size(0)
        
        for i in range(batch_size):
            # 获取原始句子
            input_ids_i = input_ids[i].cpu().tolist()
            attention_mask_i = attention_mask[i].cpu().tolist()
            tokens = self.tokenizer.convert_ids_to_tokens(input_ids_i)
            
            # 移除特殊token并转换为文本
            sentence_tokens = []
            for j, token in enumerate(tokens):
                if j == 0 or j == len(tokens) - 1:  # [CLS]和[SEP]
                    continue
                if attention_mask_i[j] == 0:  # 填充token
                    continue
                sentence_tokens.append(token)
            
            sentence = self.tokenizer.convert_tokens_to_string(sentence_tokens)
            
            # 获取预测和真实标签
            true_labels_i = true_labels[i].cpu().tolist()
            pred_labels_i = pred_labels[i].cpu().tolist()
            
            # 移除特殊token的标签
            true_labels_i = true_labels_i[1:-1]  # 移除[CLS]和[SEP]
            pred_labels_i = pred_labels_i[1:-1]
            
            # 只保留有效token的标签
            true_labels_i = true_labels_i[:len(sentence_tokens)]
            pred_labels_i = pred_labels_i[:len(sentence_tokens)]
            
            # 解码实体
            true_entities = self.decode(sentence_tokens, true_labels_i)
            pred_entities = self.decode(sentence_tokens, pred_labels_i)
            
            # 统计指标
            for key in ["PERSON", "LOCATION", "TIME", "ORGANIZATION"]:
                self.stats_dict[key]["正确识别"] += len([ent for ent in pred_entities[key] if ent in true_entities[key]])
                self.stats_dict[key]["样本实体数"] += len(true_entities[key])
                self.stats_dict[key]["识别出实体数"] += len(pred_entities[key])
    
    def show_stats(self):
        F1_scores = []
        for key in ["PERSON", "LOCATION", "TIME", "ORGANIZATION"]:
            # 正确率 = 识别出的正确实体数 / 识别出的实体数
            # 召回率 = 识别出的正确实体数 / 样本的实体数
            precision = self.stats_dict[key]["正确识别"] / (1e-5 + self.stats_dict[key]["识别出实体数"])
            recall = self.stats_dict[key]["正确识别"] / (1e-5 + self.stats_dict[key]["样本实体数"])
            F1 = (2 * precision * recall) / (precision + recall + 1e-5)
            F1_scores.append(F1)
            self.logger.info(f"{key}类实体，准确率：{precision:.6f}, 召回率: {recall:.6f}, F1: {F1:.6f}")
        
        self.logger.info(f"Macro-F1: {np.mean(F1_scores):.6f}")
        
        correct_pred = sum([self.stats_dict[key]["正确识别"] for key in ["PERSON", "LOCATION", "TIME", "ORGANIZATION"]])
        total_pred = sum([self.stats_dict[key]["识别出实体数"] for key in ["PERSON", "LOCATION", "TIME", "ORGANIZATION"]])
        true_enti = sum([self.stats_dict[key]["样本实体数"] for key in ["PERSON", "LOCATION", "TIME", "ORGANIZATION"]])
        
        micro_precision = correct_pred / (total_pred + 1e-5)
        micro_recall = correct_pred / (true_enti + 1e-5)
        micro_f1 = (2 * micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-5)
        
        self.logger.info(f"Micro-F1: {micro_f1:.6f}")
        self.logger.info("--------------------")
    
    def decode(self, tokens, labels):
        results = defaultdict(list)
        
        # 转换为IOB标签
        id2label = {
            0: "B-LOCATION",
            1: "B-ORGANIZATION",
            2: "B-PERSON",
            3: "B-TIME",
            4: "I-LOCATION",
            5: "I-ORGANIZATION",
            6: "I-PERSON",
            7: "I-TIME",
            8: "O"
        }
        
        iob_labels = [id2label.get(label, "O") for label in labels]
        
        current_entity = None
        current_type = None
        current_start = 0
        
        for i, (token, label) in enumerate(zip(tokens, iob_labels)):
            if label.startswith('B-'):
                # 开始新实体
                if current_entity is not None:
                    entity_text = self.tokenizer.convert_tokens_to_string(tokens[current_start:i])
                    results[current_type].append(entity_text)
                
                current_type = label[2:]
                current_entity = [token]
                current_start = i
            elif label.startswith('I-'):
                # 延续当前实体
                if current_entity is None or current_type != label[2:]:
                    # 不合法的I标签，当作O处理
                    if current_entity is not None:
                        entity_text = self.tokenizer.convert_tokens_to_string(tokens[current_start:i])
                        results[current_type].append(entity_text)
                    
                    current_entity = None
                    current_type = None
                else:
                    current_entity.append(token)
            else:
                # O标签
                if current_entity is not None:
                    entity_text = self.tokenizer.convert_tokens_to_string(tokens[current_start:i])
                    results[current_type].append(entity_text)
                
                current_entity = None
                current_type = None
        
        # 处理最后一个实体
        if current_entity is not None:
            entity_text = self.tokenizer.convert_tokens_to_string(tokens[current_start:])
            results[current_type].append(entity_text)
        
        return results

# 训练函数示例
def train(config, logger):
    # 初始化tokenizer和模型
    tokenizer = BertTokenizer.from_pretrained(config["model_name"])
    model = BertNERModel(num_labels=9, model_name=config["model_name"])
    
    if torch.cuda.is_available():
        model = model.cuda()
    
    # 加载训练数据
    train_data = load_data(config["train_data_path"], config, tokenizer)
    
    # 定义优化器和损失函数
    optimizer = torch.optim.AdamW(model.parameters(), lr=config["learning_rate"])
    
    # 训练循环
    for epoch in range(config["num_epochs"]):
        model.train()
        total_loss = 0
        
        for batch_data in train_data:
            if torch.cuda.is_available():
                batch_data = {k: v.cuda() for k, v in batch_data.items()}
            
            optimizer.zero_grad()
            
            outputs = model(**batch_data)
            loss = outputs[0]
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        logger.info(f"Epoch {epoch+1}/{config['num_epochs']}, Loss: {total_loss/len(train_data):.4f}")
        
        # 评估模型
        evaluator = Evaluator(config, logger)
        evaluator.eval(epoch+1)
        
        # 保存模型
        model_path = f"{config['output_dir']}/model_epoch_{epoch+1}.bin"
        torch.save(model.state_dict(), model_path)
        logger.info(f"模型已保存至: {model_path}")

# 配置示例
def get_config():
    config = {
        "model_name": "bert-base-chinese",
        "train_data_path": "data/train.txt",
        "valid_data_path": "data/valid.txt",
        "output_dir": "saved_models",
        "model_path": None,  # 若有预训练模型，可指定路径
        "max_length": 128,
        "batch_size": 16,
        "learning_rate": 2e-5,
        "num_epochs": 5,
        "use_crf": False  # BERT模型不使用CRF
    }
    return config

# 简单的日志记录器
class SimpleLogger:
    def __init__(self, log_file=None):
        self.log_file = log_file
    
    def info(self, message):
        print(message)
        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(message + '\n')

if __name__ == "__main__":
    config = get_config()
    logger = SimpleLogger("train.log")
    
    # 训练模型
    train(config, logger)
    
    # 评估模型
    evaluator = Evaluator(config, logger)
    evaluator.eval()
